{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ailabguest/anaconda3/envs/babble\n",
      "fatal: destination path 'Informer2020' already exists and is not an empty directory.\n",
      "fatal: destination path 'ETDataset' already exists and is not an empty directory.\n",
      "create-csv.ipynb      informer.ipynb\t\t    output-with-date.csv\n",
      "ETDataset\t      output.csv\t\t    results\n",
      "Informer2020\t      output-with-date2.csv\t    time-epoc.py\n",
      "informer_checkpoints  output-with-date-average.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.prefix)\n",
    "!git clone https://github.com/zhouhaoyi/Informer2020.git\n",
    "!git clone https://github.com/zhouhaoyi/ETDataset.git\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'Informer2020' in sys.path:\n",
    "    sys.path += ['Informer2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.1.1\n",
      "  Downloading matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.19.4\n",
      "  Downloading numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.25.1\n",
      "  Downloading pandas-0.25.1-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit_learn==0.21.3\n",
      "  Downloading scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.5 MB 5.4 kB/s  eta 0:00:01     |█████▉                          | 135.3 MB 12.1 MB/s eta 0:00:50     |███████████▊                    | 270.5 MB 10.7 MB/s eta 0:00:44     |██████████████████████████▏     | 600.9 MB 12.1 MB/s eta 0:00:12\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from pandas==0.25.1->-r ./Informer2020/requirements.txt (line 3)) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from scikit_learn==0.21.3->-r ./Informer2020/requirements.txt (line 4)) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from scikit_learn==0.21.3->-r ./Informer2020/requirements.txt (line 4)) (1.4.1)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (45.1.0.post20200119)\n",
      "Requirement already satisfied: six in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from cycler>=0.10->matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (1.14.0)\n",
      "Installing collected packages: numpy, matplotlib, pandas, scikit-learn, dataclasses, typing-extensions, torch\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.17.5\n",
      "    Uninstalling numpy-1.17.5:\n",
      "      Successfully uninstalled numpy-1.17.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.1.3\n",
      "    Uninstalling matplotlib-3.1.3:\n",
      "      Successfully uninstalled matplotlib-3.1.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.0\n",
      "    Uninstalling pandas-1.0.0:\n",
      "      Successfully uninstalled pandas-1.0.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "Successfully installed dataclasses-0.8 matplotlib-3.1.1 numpy-1.19.4 pandas-0.25.1 scikit-learn-0.21.3 torch-1.8.0 typing-extensions-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -r ./Informer2020/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'custom' # data\n",
    "args.root_path = './' # root path of data file\n",
    "args.data_path = 'normalized-pressure-1000.csv' # data file\n",
    "args.features = 'S' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'pressure_normalized' # target feature in S or MS task\n",
    "args.freq = 's' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 96#mod 96 # input sequence length of Informer encoder\n",
    "args.label_len = 48#mod 48 # start token length of Informer decoder\n",
    "args.pred_len = 24#mod 24 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 1 # encoder input size\n",
    "args.dec_in = 1 # decoder input size\n",
    "args.c_out = 1 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512#modified # dimension of model\n",
    "args.n_heads = 8#modified 8 # num of heads\n",
    "args.e_layers = 2#modified 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048#modified 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "\n",
    "args.batch_size = 32 #modified 32\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 6#modified\n",
    "args.patience = 3#modified\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set augments by using data name\n",
    "data_parser = {\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informer', 'data': 'custom', 'root_path': './', 'data_path': 'normalized-pressure-1000.csv', 'features': 'S', 'target': 'pressure_normalized', 'freq': 's', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 6, 'patience': 3, 'des': 'exp', 'use_gpu': False, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 's'}\n",
      "Max epochs: 6\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)\n",
    "print(f\"Max epochs: {args.train_epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : informer_custom_ftS_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8482\n",
      "val 1207\n",
      "test 2434\n",
      "\titers: 100, epoch: 1 | loss: 0.9769275\n",
      "\tspeed: 1.0133s/iter; left time: 1510.7933s\n",
      "\titers: 200, epoch: 1 | loss: 0.9905620\n",
      "\tspeed: 1.0183s/iter; left time: 1416.3877s\n",
      "Epoch: 1 cost time: 269.8800482749939\n",
      "Epoch: 1, Steps: 265 | Train Loss: 1.0270644 Vali Loss: 1.0531771 Test Loss: 1.0878022\n",
      "Validation loss decreased (inf --> 1.053177).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 1.0506228\n",
      "\tspeed: 2.0964s/iter; left time: 2570.2468s\n",
      "\titers: 200, epoch: 2 | loss: 0.8846354\n",
      "\tspeed: 1.0160s/iter; left time: 1143.9832s\n",
      "Epoch: 2 cost time: 269.302433013916\n",
      "Epoch: 2, Steps: 265 | Train Loss: 0.9979007 Vali Loss: 1.0488914 Test Loss: 1.0808892\n",
      "Validation loss decreased (1.053177 --> 1.048891).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 1.0022402\n",
      "\tspeed: 2.0937s/iter; left time: 2012.0264s\n",
      "\titers: 200, epoch: 3 | loss: 0.9243598\n",
      "\tspeed: 1.0195s/iter; left time: 877.7535s\n",
      "Epoch: 3 cost time: 269.9466977119446\n",
      "Epoch: 3, Steps: 265 | Train Loss: 0.9900227 Vali Loss: 1.0421991 Test Loss: 1.0759062\n",
      "Validation loss decreased (1.048891 --> 1.042199).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.8911805\n",
      "\tspeed: 2.0927s/iter; left time: 1456.5285s\n",
      "\titers: 200, epoch: 4 | loss: 1.0003024\n",
      "\tspeed: 1.0167s/iter; left time: 605.9501s\n",
      "Epoch: 4 cost time: 269.14991426467896\n",
      "Epoch: 4, Steps: 265 | Train Loss: 0.9855314 Vali Loss: 1.0406563 Test Loss: 1.0716664\n",
      "Validation loss decreased (1.042199 --> 1.040656).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.9530401\n",
      "\tspeed: 2.0953s/iter; left time: 903.0874s\n",
      "\titers: 200, epoch: 5 | loss: 0.9702105\n",
      "\tspeed: 1.0168s/iter; left time: 336.5590s\n",
      "Epoch: 5 cost time: 269.7280418872833\n",
      "Epoch: 5, Steps: 265 | Train Loss: 0.9832358 Vali Loss: 1.0383058 Test Loss: 1.0702289\n",
      "Validation loss decreased (1.040656 --> 1.038306).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 1.1396807\n",
      "\tspeed: 2.0975s/iter; left time: 348.1806s\n",
      "\titers: 200, epoch: 6 | loss: 0.9818802\n",
      "\tspeed: 1.0154s/iter; left time: 67.0154s\n",
      "Epoch: 6 cost time: 269.624698638916\n",
      "Epoch: 6, Steps: 265 | Train Loss: 0.9817598 Vali Loss: 1.0362642 Test Loss: 1.0697650\n",
      "Validation loss decreased (1.038306 --> 1.036264).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      ">>>>>>>testing : informer_custom_ftS_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2434\n",
      "test shape: (76, 32, 24, 1) (76, 32, 24, 1)\n",
      "test shape: (2432, 24, 1) (2432, 24, 1)\n",
      "mse:1.069826602935791, mae:0.8179079294204712\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_loader import Dataset_Custom\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# args.target = 'HULL'\n",
    "# args.freq = 'h'\n",
    "\n",
    "Data = Dataset_Custom\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'test'; shuffle_flag = False; drop_last = True; batch_size = 1\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    timeenc=timeenc,\n",
    "    target=args.target, # HULL here\n",
    "    freq=args.freq # 'h': hourly, 't':minutely\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
