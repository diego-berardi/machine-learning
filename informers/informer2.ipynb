{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ailabguest/anaconda3/envs/babble\n",
      "fatal: destination path 'Informer2020' already exists and is not an empty directory.\n",
      "fatal: destination path 'ETDataset' already exists and is not an empty directory.\n",
      "create-csv.ipynb      informer.ipynb\t\t    output-with-date.csv\n",
      "ETDataset\t      output.csv\t\t    results\n",
      "Informer2020\t      output-with-date2.csv\t    time-epoc.py\n",
      "informer_checkpoints  output-with-date-average.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.prefix)\n",
    "!git clone https://github.com/zhouhaoyi/Informer2020.git\n",
    "!git clone https://github.com/zhouhaoyi/ETDataset.git\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'Informer2020' in sys.path:\n",
    "    sys.path += ['Informer2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.1.1\n",
      "  Downloading matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.19.4\n",
      "  Downloading numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.25.1\n",
      "  Downloading pandas-0.25.1-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit_learn==0.21.3\n",
      "  Downloading scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.5 MB 5.4 kB/s  eta 0:00:01     |█████▉                          | 135.3 MB 12.1 MB/s eta 0:00:50     |███████████▊                    | 270.5 MB 10.7 MB/s eta 0:00:44     |██████████████████████████▏     | 600.9 MB 12.1 MB/s eta 0:00:12\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from pandas==0.25.1->-r ./Informer2020/requirements.txt (line 3)) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from scikit_learn==0.21.3->-r ./Informer2020/requirements.txt (line 4)) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from scikit_learn==0.21.3->-r ./Informer2020/requirements.txt (line 4)) (1.4.1)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (45.1.0.post20200119)\n",
      "Requirement already satisfied: six in /home/ailabguest/anaconda3/envs/babble/lib/python3.6/site-packages (from cycler>=0.10->matplotlib==3.1.1->-r ./Informer2020/requirements.txt (line 1)) (1.14.0)\n",
      "Installing collected packages: numpy, matplotlib, pandas, scikit-learn, dataclasses, typing-extensions, torch\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.17.5\n",
      "    Uninstalling numpy-1.17.5:\n",
      "      Successfully uninstalled numpy-1.17.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.1.3\n",
      "    Uninstalling matplotlib-3.1.3:\n",
      "      Successfully uninstalled matplotlib-3.1.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.0\n",
      "    Uninstalling pandas-1.0.0:\n",
      "      Successfully uninstalled pandas-1.0.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "Successfully installed dataclasses-0.8 matplotlib-3.1.1 numpy-1.19.4 pandas-0.25.1 scikit-learn-0.21.3 torch-1.8.0 typing-extensions-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -r ./Informer2020/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'custom' # data\n",
    "args.root_path = './' # root path of data file\n",
    "args.data_path = 'output-with-date.csv' # data file\n",
    "args.features = 'S' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'pressure' # target feature in S or MS task\n",
    "args.freq = 's' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 48#mod 96 # input sequence length of Informer encoder\n",
    "args.label_len = 24#mod 48 # start token length of Informer decoder\n",
    "args.pred_len = 12#mod 24 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 1 # encoder input size\n",
    "args.dec_in = 1 # decoder input size\n",
    "args.c_out = 1 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 256#modified # dimension of model\n",
    "args.n_heads = 1#modified 8 # num of heads\n",
    "args.e_layers = 1#modified 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 1024#modified 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "\n",
    "args.batch_size = 16 #modified 32\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 3#modified\n",
    "args.patience = 1#modified\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set augments by using data name\n",
    "data_parser = {\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informer', 'data': 'custom', 'root_path': './', 'data_path': 'output-with-date.csv', 'features': 'S', 'target': 'pressure', 'freq': 's', 'checkpoints': './informer_checkpoints', 'seq_len': 48, 'label_len': 24, 'pred_len': 12, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 256, 'n_heads': 1, 'e_layers': 1, 'd_layers': 1, 'd_ff': 1024, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 16, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 3, 'patience': 1, 'des': 'exp', 'use_gpu': False, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 's'}\n",
      "Max epochs: 3\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)\n",
    "print(f\"Max epochs: {args.train_epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : informer_custom_ftS_sl48_ll24_pl12_dm256_nh1_el1_dl1_df1024_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 85957\n",
      "val 12277\n",
      "test 24565\n",
      "\titers: 100, epoch: 1 | loss: 1.0268863\n",
      "\tspeed: 0.1141s/iter; left time: 1826.7800s\n",
      "\titers: 200, epoch: 1 | loss: 0.9277111\n",
      "\tspeed: 0.1158s/iter; left time: 1843.2594s\n",
      "\titers: 300, epoch: 1 | loss: 1.0305418\n",
      "\tspeed: 0.1164s/iter; left time: 1840.9387s\n",
      "\titers: 400, epoch: 1 | loss: 0.9032533\n",
      "\tspeed: 0.1158s/iter; left time: 1820.6007s\n",
      "\titers: 500, epoch: 1 | loss: 1.0470827\n",
      "\tspeed: 0.1162s/iter; left time: 1815.1579s\n",
      "\titers: 600, epoch: 1 | loss: 1.0090394\n",
      "\tspeed: 0.1163s/iter; left time: 1804.4953s\n",
      "\titers: 700, epoch: 1 | loss: 0.9071641\n",
      "\tspeed: 0.1153s/iter; left time: 1777.2423s\n",
      "\titers: 800, epoch: 1 | loss: 0.9319744\n",
      "\tspeed: 0.1157s/iter; left time: 1771.7945s\n",
      "\titers: 900, epoch: 1 | loss: 1.1203487\n",
      "\tspeed: 0.1160s/iter; left time: 1765.9133s\n",
      "\titers: 1000, epoch: 1 | loss: 1.0979403\n",
      "\tspeed: 0.1176s/iter; left time: 1778.1248s\n",
      "\titers: 1100, epoch: 1 | loss: 0.9180737\n",
      "\tspeed: 0.1166s/iter; left time: 1751.4276s\n",
      "\titers: 1200, epoch: 1 | loss: 1.0516616\n",
      "\tspeed: 0.1140s/iter; left time: 1699.9131s\n",
      "\titers: 1300, epoch: 1 | loss: 0.9034958\n",
      "\tspeed: 0.1151s/iter; left time: 1705.8704s\n",
      "\titers: 1400, epoch: 1 | loss: 0.9383531\n",
      "\tspeed: 0.1160s/iter; left time: 1707.5322s\n",
      "\titers: 1500, epoch: 1 | loss: 0.9395291\n",
      "\tspeed: 0.1147s/iter; left time: 1676.1228s\n",
      "\titers: 1600, epoch: 1 | loss: 1.0731980\n",
      "\tspeed: 0.1144s/iter; left time: 1660.6950s\n",
      "\titers: 1700, epoch: 1 | loss: 1.0525328\n",
      "\tspeed: 0.1154s/iter; left time: 1664.3698s\n",
      "\titers: 1800, epoch: 1 | loss: 0.8672276\n",
      "\tspeed: 0.1154s/iter; left time: 1651.5305s\n",
      "\titers: 1900, epoch: 1 | loss: 1.0478830\n",
      "\tspeed: 0.1150s/iter; left time: 1635.6045s\n",
      "\titers: 2000, epoch: 1 | loss: 1.0046692\n",
      "\tspeed: 0.1159s/iter; left time: 1635.8490s\n",
      "\titers: 2100, epoch: 1 | loss: 1.1447879\n",
      "\tspeed: 0.1165s/iter; left time: 1633.2164s\n",
      "\titers: 2200, epoch: 1 | loss: 1.0709914\n",
      "\tspeed: 0.1166s/iter; left time: 1622.1412s\n",
      "\titers: 2300, epoch: 1 | loss: 1.1833658\n",
      "\tspeed: 0.1172s/iter; left time: 1618.8323s\n",
      "\titers: 2400, epoch: 1 | loss: 0.9899549\n",
      "\tspeed: 0.1154s/iter; left time: 1583.2545s\n",
      "\titers: 2500, epoch: 1 | loss: 0.9869881\n",
      "\tspeed: 0.1173s/iter; left time: 1597.5855s\n",
      "\titers: 2600, epoch: 1 | loss: 0.7703283\n",
      "\tspeed: 0.1157s/iter; left time: 1563.9567s\n",
      "\titers: 2700, epoch: 1 | loss: 0.9489920\n",
      "\tspeed: 0.1169s/iter; left time: 1568.4807s\n",
      "\titers: 2800, epoch: 1 | loss: 0.9529395\n",
      "\tspeed: 0.1158s/iter; left time: 1541.5837s\n",
      "\titers: 2900, epoch: 1 | loss: 0.8264167\n",
      "\tspeed: 0.1158s/iter; left time: 1530.6030s\n",
      "\titers: 3000, epoch: 1 | loss: 0.8897288\n",
      "\tspeed: 0.1160s/iter; left time: 1521.0883s\n",
      "\titers: 3100, epoch: 1 | loss: 0.8109941\n",
      "\tspeed: 0.1156s/iter; left time: 1504.1928s\n",
      "\titers: 3200, epoch: 1 | loss: 1.0828128\n",
      "\tspeed: 0.1170s/iter; left time: 1511.0870s\n",
      "\titers: 3300, epoch: 1 | loss: 1.0212173\n",
      "\tspeed: 0.1164s/iter; left time: 1492.3384s\n",
      "\titers: 3400, epoch: 1 | loss: 0.9469995\n",
      "\tspeed: 0.1159s/iter; left time: 1473.9023s\n",
      "\titers: 3500, epoch: 1 | loss: 1.1136132\n",
      "\tspeed: 0.1157s/iter; left time: 1459.9993s\n",
      "\titers: 3600, epoch: 1 | loss: 1.1862326\n",
      "\tspeed: 0.1144s/iter; left time: 1431.4904s\n",
      "\titers: 3700, epoch: 1 | loss: 1.1747628\n",
      "\tspeed: 0.1168s/iter; left time: 1450.2770s\n",
      "\titers: 3800, epoch: 1 | loss: 1.0739805\n",
      "\tspeed: 0.1165s/iter; left time: 1434.9348s\n",
      "\titers: 3900, epoch: 1 | loss: 1.1682185\n",
      "\tspeed: 0.1141s/iter; left time: 1393.7319s\n",
      "\titers: 4000, epoch: 1 | loss: 0.8800442\n",
      "\tspeed: 0.1181s/iter; left time: 1430.7777s\n",
      "\titers: 4100, epoch: 1 | loss: 0.8699567\n",
      "\tspeed: 0.1186s/iter; left time: 1425.5486s\n",
      "\titers: 4200, epoch: 1 | loss: 0.9345422\n",
      "\tspeed: 0.1149s/iter; left time: 1369.3621s\n",
      "\titers: 4300, epoch: 1 | loss: 1.3602971\n",
      "\tspeed: 0.1177s/iter; left time: 1391.2571s\n",
      "\titers: 4400, epoch: 1 | loss: 1.0348846\n",
      "\tspeed: 0.1155s/iter; left time: 1353.0121s\n",
      "\titers: 4500, epoch: 1 | loss: 0.8720732\n",
      "\tspeed: 0.1154s/iter; left time: 1341.0209s\n",
      "\titers: 4600, epoch: 1 | loss: 0.8994094\n",
      "\tspeed: 0.1185s/iter; left time: 1364.1903s\n",
      "\titers: 4700, epoch: 1 | loss: 1.1252029\n",
      "\tspeed: 0.1158s/iter; left time: 1322.6429s\n",
      "\titers: 4800, epoch: 1 | loss: 0.8503549\n",
      "\tspeed: 0.1143s/iter; left time: 1293.7444s\n",
      "\titers: 4900, epoch: 1 | loss: 0.9789498\n",
      "\tspeed: 0.1168s/iter; left time: 1310.0843s\n",
      "\titers: 5000, epoch: 1 | loss: 1.0425454\n",
      "\tspeed: 0.1164s/iter; left time: 1293.7398s\n",
      "\titers: 5100, epoch: 1 | loss: 1.1710203\n",
      "\tspeed: 0.1167s/iter; left time: 1286.1905s\n",
      "\titers: 5200, epoch: 1 | loss: 1.1888982\n",
      "\tspeed: 0.1147s/iter; left time: 1251.9521s\n",
      "\titers: 5300, epoch: 1 | loss: 1.0782548\n",
      "\tspeed: 0.1166s/iter; left time: 1261.0646s\n",
      "Epoch: 1 cost time: 623.1139032840729\n",
      "Epoch: 1, Steps: 5372 | Train Loss: 1.0031617 Vali Loss: 0.9835448 Test Loss: 1.0299578\n",
      "Validation loss decreased (inf --> 0.983545).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.9588151\n",
      "\tspeed: 0.9263s/iter; left time: 9860.6106s\n",
      "\titers: 200, epoch: 2 | loss: 1.1208129\n",
      "\tspeed: 0.1094s/iter; left time: 1154.0566s\n",
      "\titers: 300, epoch: 2 | loss: 1.1849797\n",
      "\tspeed: 0.1102s/iter; left time: 1151.5259s\n",
      "\titers: 400, epoch: 2 | loss: 1.1347939\n",
      "\tspeed: 0.1121s/iter; left time: 1159.6858s\n",
      "\titers: 500, epoch: 2 | loss: 0.9503114\n",
      "\tspeed: 0.1092s/iter; left time: 1118.3752s\n",
      "\titers: 600, epoch: 2 | loss: 0.9925544\n",
      "\tspeed: 0.1090s/iter; left time: 1106.2072s\n",
      "\titers: 700, epoch: 2 | loss: 1.2485830\n",
      "\tspeed: 0.1094s/iter; left time: 1098.8632s\n",
      "\titers: 800, epoch: 2 | loss: 1.2430114\n",
      "\tspeed: 0.1117s/iter; left time: 1111.0488s\n",
      "\titers: 900, epoch: 2 | loss: 1.1990613\n",
      "\tspeed: 0.1093s/iter; left time: 1076.4039s\n",
      "\titers: 1000, epoch: 2 | loss: 0.9383397\n",
      "\tspeed: 0.1110s/iter; left time: 1081.3756s\n",
      "\titers: 1100, epoch: 2 | loss: 0.8313192\n",
      "\tspeed: 0.1102s/iter; left time: 1062.7221s\n",
      "\titers: 1200, epoch: 2 | loss: 1.0707822\n",
      "\tspeed: 0.1106s/iter; left time: 1055.3366s\n",
      "\titers: 1300, epoch: 2 | loss: 0.8872332\n",
      "\tspeed: 0.1117s/iter; left time: 1054.6516s\n",
      "\titers: 1400, epoch: 2 | loss: 0.9375431\n",
      "\tspeed: 0.1105s/iter; left time: 1032.2984s\n",
      "\titers: 1500, epoch: 2 | loss: 1.1509004\n",
      "\tspeed: 0.1110s/iter; left time: 1026.3495s\n",
      "\titers: 1600, epoch: 2 | loss: 1.0780069\n",
      "\tspeed: 0.1083s/iter; left time: 990.1802s\n",
      "\titers: 1700, epoch: 2 | loss: 0.8811228\n",
      "\tspeed: 0.1119s/iter; left time: 1012.1747s\n",
      "\titers: 1800, epoch: 2 | loss: 1.0300723\n",
      "\tspeed: 0.1116s/iter; left time: 998.1967s\n",
      "\titers: 1900, epoch: 2 | loss: 0.7575765\n",
      "\tspeed: 0.1111s/iter; left time: 983.1049s\n",
      "\titers: 2000, epoch: 2 | loss: 1.0687457\n",
      "\tspeed: 0.1108s/iter; left time: 968.7503s\n",
      "\titers: 2100, epoch: 2 | loss: 1.1357998\n",
      "\tspeed: 0.1116s/iter; left time: 964.8694s\n",
      "\titers: 2200, epoch: 2 | loss: 0.8902661\n",
      "\tspeed: 0.1099s/iter; left time: 939.0532s\n",
      "\titers: 2300, epoch: 2 | loss: 0.8915595\n",
      "\tspeed: 0.1100s/iter; left time: 929.1518s\n",
      "\titers: 2400, epoch: 2 | loss: 1.0637622\n",
      "\tspeed: 0.1106s/iter; left time: 923.1960s\n",
      "\titers: 2500, epoch: 2 | loss: 0.8346014\n",
      "\tspeed: 0.1098s/iter; left time: 905.6331s\n",
      "\titers: 2600, epoch: 2 | loss: 0.9714131\n",
      "\tspeed: 0.1125s/iter; left time: 916.5104s\n",
      "\titers: 2700, epoch: 2 | loss: 1.1244489\n",
      "\tspeed: 0.1127s/iter; left time: 906.4287s\n",
      "\titers: 2800, epoch: 2 | loss: 0.8779444\n",
      "\tspeed: 0.1104s/iter; left time: 877.3484s\n",
      "\titers: 2900, epoch: 2 | loss: 0.8220289\n",
      "\tspeed: 0.1122s/iter; left time: 880.1818s\n",
      "\titers: 3000, epoch: 2 | loss: 0.9885210\n",
      "\tspeed: 0.1122s/iter; left time: 869.1530s\n",
      "\titers: 3100, epoch: 2 | loss: 0.8925381\n",
      "\tspeed: 0.1130s/iter; left time: 863.6149s\n",
      "\titers: 3200, epoch: 2 | loss: 0.9888111\n",
      "\tspeed: 0.1131s/iter; left time: 853.1600s\n",
      "\titers: 3300, epoch: 2 | loss: 0.9122481\n",
      "\tspeed: 0.1157s/iter; left time: 861.5797s\n",
      "\titers: 3400, epoch: 2 | loss: 0.9573751\n",
      "\tspeed: 0.1151s/iter; left time: 845.3945s\n",
      "\titers: 3500, epoch: 2 | loss: 0.9959378\n",
      "\tspeed: 0.1162s/iter; left time: 842.2134s\n",
      "\titers: 3600, epoch: 2 | loss: 1.1011884\n",
      "\tspeed: 0.1164s/iter; left time: 831.9657s\n",
      "\titers: 3700, epoch: 2 | loss: 1.1854120\n",
      "\tspeed: 0.1181s/iter; left time: 831.8473s\n",
      "\titers: 3800, epoch: 2 | loss: 0.9652455\n",
      "\tspeed: 0.1186s/iter; left time: 823.3765s\n",
      "\titers: 3900, epoch: 2 | loss: 1.0178930\n",
      "\tspeed: 0.1192s/iter; left time: 815.6968s\n",
      "\titers: 4000, epoch: 2 | loss: 1.1781377\n",
      "\tspeed: 0.1194s/iter; left time: 805.4531s\n",
      "\titers: 4100, epoch: 2 | loss: 0.9363727\n",
      "\tspeed: 0.1201s/iter; left time: 797.9231s\n",
      "\titers: 4200, epoch: 2 | loss: 0.9530025\n",
      "\tspeed: 0.1187s/iter; left time: 776.7474s\n",
      "\titers: 4300, epoch: 2 | loss: 0.9241896\n",
      "\tspeed: 0.1214s/iter; left time: 782.2880s\n",
      "\titers: 4400, epoch: 2 | loss: 1.0895275\n",
      "\tspeed: 0.1324s/iter; left time: 840.2631s\n",
      "\titers: 4500, epoch: 2 | loss: 1.0811517\n",
      "\tspeed: 0.1362s/iter; left time: 850.3086s\n",
      "\titers: 4600, epoch: 2 | loss: 0.9449296\n",
      "\tspeed: 0.1333s/iter; left time: 819.3859s\n",
      "\titers: 4700, epoch: 2 | loss: 1.2349933\n",
      "\tspeed: 0.1312s/iter; left time: 793.0609s\n",
      "\titers: 4800, epoch: 2 | loss: 1.1795038\n",
      "\tspeed: 0.1340s/iter; left time: 796.5039s\n",
      "\titers: 4900, epoch: 2 | loss: 0.8225629\n",
      "\tspeed: 0.1343s/iter; left time: 785.2591s\n",
      "\titers: 5000, epoch: 2 | loss: 1.0284824\n",
      "\tspeed: 0.1331s/iter; left time: 764.8742s\n",
      "\titers: 5100, epoch: 2 | loss: 1.1059114\n",
      "\tspeed: 0.1329s/iter; left time: 750.2047s\n",
      "\titers: 5200, epoch: 2 | loss: 1.2441685\n",
      "\tspeed: 0.1328s/iter; left time: 736.1231s\n",
      "\titers: 5300, epoch: 2 | loss: 1.2077624\n",
      "\tspeed: 0.1320s/iter; left time: 718.9541s\n",
      "Epoch: 2 cost time: 627.5346636772156\n",
      "Epoch: 2, Steps: 5372 | Train Loss: 1.0006093 Vali Loss: 0.9827482 Test Loss: 1.0298274\n",
      "Validation loss decreased (0.983545 --> 0.982748).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 1.0832232\n",
      "\tspeed: 1.0037s/iter; left time: 5292.3185s\n",
      "\titers: 200, epoch: 3 | loss: 1.1271781\n",
      "\tspeed: 0.1330s/iter; left time: 688.0336s\n",
      "\titers: 300, epoch: 3 | loss: 0.9961764\n",
      "\tspeed: 0.1328s/iter; left time: 673.6788s\n",
      "\titers: 400, epoch: 3 | loss: 1.1418395\n",
      "\tspeed: 0.1332s/iter; left time: 662.4260s\n",
      "\titers: 500, epoch: 3 | loss: 0.9860704\n",
      "\tspeed: 0.1327s/iter; left time: 646.5000s\n",
      "\titers: 600, epoch: 3 | loss: 0.9862141\n",
      "\tspeed: 0.1352s/iter; left time: 645.4035s\n",
      "\titers: 700, epoch: 3 | loss: 0.9231561\n",
      "\tspeed: 0.1343s/iter; left time: 627.4705s\n",
      "\titers: 800, epoch: 3 | loss: 0.9777023\n",
      "\tspeed: 0.1326s/iter; left time: 606.2711s\n",
      "\titers: 900, epoch: 3 | loss: 1.1697304\n",
      "\tspeed: 0.1330s/iter; left time: 594.8448s\n",
      "\titers: 1000, epoch: 3 | loss: 1.1876851\n",
      "\tspeed: 0.1331s/iter; left time: 582.0300s\n",
      "\titers: 1100, epoch: 3 | loss: 1.0130917\n",
      "\tspeed: 0.1332s/iter; left time: 569.1410s\n",
      "\titers: 1200, epoch: 3 | loss: 0.6813469\n",
      "\tspeed: 0.1321s/iter; left time: 551.3240s\n",
      "\titers: 1300, epoch: 3 | loss: 1.2556779\n",
      "\tspeed: 0.1329s/iter; left time: 541.1129s\n",
      "\titers: 1400, epoch: 3 | loss: 1.0459846\n",
      "\tspeed: 0.1359s/iter; left time: 540.0363s\n",
      "\titers: 1500, epoch: 3 | loss: 1.1133782\n",
      "\tspeed: 0.1334s/iter; left time: 516.7057s\n",
      "\titers: 1600, epoch: 3 | loss: 0.9716384\n",
      "\tspeed: 0.1332s/iter; left time: 502.5076s\n",
      "\titers: 1700, epoch: 3 | loss: 1.0276583\n",
      "\tspeed: 0.1332s/iter; left time: 489.1280s\n",
      "\titers: 1800, epoch: 3 | loss: 0.8274074\n",
      "\tspeed: 0.1319s/iter; left time: 471.1478s\n",
      "\titers: 1900, epoch: 3 | loss: 1.0574936\n",
      "\tspeed: 0.1339s/iter; left time: 465.1551s\n",
      "\titers: 2000, epoch: 3 | loss: 0.7198424\n",
      "\tspeed: 0.1336s/iter; left time: 450.6657s\n",
      "\titers: 2100, epoch: 3 | loss: 1.0909947\n",
      "\tspeed: 0.1334s/iter; left time: 436.6189s\n",
      "\titers: 2200, epoch: 3 | loss: 0.9111386\n",
      "\tspeed: 0.1357s/iter; left time: 430.5006s\n",
      "\titers: 2300, epoch: 3 | loss: 0.9140407\n",
      "\tspeed: 0.1336s/iter; left time: 410.6454s\n",
      "\titers: 2400, epoch: 3 | loss: 0.9042976\n",
      "\tspeed: 0.1323s/iter; left time: 393.2822s\n",
      "\titers: 2500, epoch: 3 | loss: 1.2034360\n",
      "\tspeed: 0.1348s/iter; left time: 387.2330s\n",
      "\titers: 2600, epoch: 3 | loss: 0.9399600\n",
      "\tspeed: 0.1320s/iter; left time: 366.0105s\n",
      "\titers: 2700, epoch: 3 | loss: 0.9522633\n",
      "\tspeed: 0.1366s/iter; left time: 365.1435s\n",
      "\titers: 2800, epoch: 3 | loss: 0.9555575\n",
      "\tspeed: 0.1343s/iter; left time: 345.6560s\n",
      "\titers: 2900, epoch: 3 | loss: 1.0247157\n",
      "\tspeed: 0.1355s/iter; left time: 335.1164s\n",
      "\titers: 3000, epoch: 3 | loss: 1.2607871\n",
      "\tspeed: 0.1364s/iter; left time: 323.6167s\n",
      "\titers: 3100, epoch: 3 | loss: 0.9570104\n",
      "\tspeed: 0.1332s/iter; left time: 302.7313s\n",
      "\titers: 3200, epoch: 3 | loss: 1.0471158\n",
      "\tspeed: 0.1349s/iter; left time: 293.2202s\n",
      "\titers: 3300, epoch: 3 | loss: 0.9555265\n",
      "\tspeed: 0.1338s/iter; left time: 277.3198s\n",
      "\titers: 3400, epoch: 3 | loss: 0.9177434\n",
      "\tspeed: 0.1321s/iter; left time: 260.7195s\n",
      "\titers: 3500, epoch: 3 | loss: 0.7081110\n",
      "\tspeed: 0.1331s/iter; left time: 249.3320s\n",
      "\titers: 3600, epoch: 3 | loss: 1.1389924\n",
      "\tspeed: 0.1318s/iter; left time: 233.7570s\n",
      "\titers: 3700, epoch: 3 | loss: 0.8916556\n",
      "\tspeed: 0.1332s/iter; left time: 222.7600s\n",
      "\titers: 3800, epoch: 3 | loss: 0.9838297\n",
      "\tspeed: 0.1322s/iter; left time: 208.0087s\n",
      "\titers: 3900, epoch: 3 | loss: 1.0347139\n",
      "\tspeed: 0.1300s/iter; left time: 191.4968s\n",
      "\titers: 4000, epoch: 3 | loss: 1.0362059\n",
      "\tspeed: 0.1279s/iter; left time: 175.6220s\n",
      "\titers: 4100, epoch: 3 | loss: 1.0767441\n",
      "\tspeed: 0.1319s/iter; left time: 167.8686s\n",
      "\titers: 4200, epoch: 3 | loss: 1.2962860\n",
      "\tspeed: 0.1320s/iter; left time: 154.7936s\n",
      "\titers: 4300, epoch: 3 | loss: 0.8059187\n",
      "\tspeed: 0.1311s/iter; left time: 140.6533s\n",
      "\titers: 4400, epoch: 3 | loss: 1.1395065\n",
      "\tspeed: 0.1316s/iter; left time: 128.0559s\n",
      "\titers: 4500, epoch: 3 | loss: 1.0143083\n",
      "\tspeed: 0.1349s/iter; left time: 117.7707s\n",
      "\titers: 4600, epoch: 3 | loss: 1.0130831\n",
      "\tspeed: 0.1299s/iter; left time: 100.3902s\n",
      "\titers: 4700, epoch: 3 | loss: 0.9931512\n",
      "\tspeed: 0.1346s/iter; left time: 90.5665s\n",
      "\titers: 4800, epoch: 3 | loss: 1.1155665\n",
      "\tspeed: 0.1332s/iter; left time: 76.3223s\n",
      "\titers: 4900, epoch: 3 | loss: 0.8101056\n",
      "\tspeed: 0.1314s/iter; left time: 62.1375s\n",
      "\titers: 5000, epoch: 3 | loss: 1.0702733\n",
      "\tspeed: 0.1309s/iter; left time: 48.8192s\n",
      "\titers: 5100, epoch: 3 | loss: 0.7677321\n",
      "\tspeed: 0.1302s/iter; left time: 35.5368s\n",
      "\titers: 5200, epoch: 3 | loss: 0.9393190\n",
      "\tspeed: 0.1321s/iter; left time: 22.8515s\n",
      "\titers: 5300, epoch: 3 | loss: 1.0243746\n",
      "\tspeed: 0.1328s/iter; left time: 9.6933s\n",
      "Epoch: 3 cost time: 714.409096956253\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_loader import Dataset_Custom\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# args.target = 'HULL'\n",
    "# args.freq = 'h'\n",
    "\n",
    "Data = Dataset_Custom\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'test'; shuffle_flag = False; drop_last = True; batch_size = 1\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    timeenc=timeenc,\n",
    "    target=args.target, # HULL here\n",
    "    freq=args.freq # 'h': hourly, 't':minutely\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
